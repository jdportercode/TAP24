{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by J.D. Porter  for the 2024 Text Analysis Pedagogy Institute, with support from [Constellate](https://constellate.org).\n",
    "<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# `Small Language Models` `3`\n",
    "\n",
    "This is lesson `3` of 3 in the educational series on `small language models`. This notebook is intended `to teach a basic strategy for building a word vector model from a small corpus of texts`. \n",
    "\n",
    "**Skills:** \n",
    "* Text analysis\n",
    "* Python\n",
    "\n",
    "**Audience:** `Teachers` / `Learners` / `Researchers`\n",
    "\n",
    "**Use case:** `Tutorial`\n",
    "\n",
    "**Difficulty:** `Beginner`\n",
    "\n",
    "**Completion time:** `90 minutes`\n",
    "\n",
    "**Knowledge Required:** \n",
    "```\n",
    "* Python basics (variables, functions, lists, dictionaries)\n",
    "```\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "```\n",
    "n/a\n",
    "```\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "```\n",
    "1. Turn collocate data about a text into an array\n",
    "2. Implement various distance measures for vectors\n",
    "3. Play with parameters for a vector model\n",
    "4. Find the most similar words to some target word\n",
    "```\n",
    "**Research Pipeline:**\n",
    "```\n",
    "If you want to try some of this on your own, you can:\n",
    "1. Gather some texts and save them on your machine as .txt files\n",
    "2. Use whatever steps you're interested in from this notebook\n",
    "3. Interpret!\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    " * To keep things simple, we will try to work with very few libraries in this notebook (just 'os', which you do not need to install, though you do need to import it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "import os\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedd148",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Required Data\n",
    "\n",
    "**Data Format:** \n",
    "* plain text (.txt)\n",
    "\n",
    "**Data Source:**\n",
    "* included files (though you may supplement these whenever you feel comfortable doing so!)\n",
    "\n",
    "**Data Quality/Bias:**\n",
    "\n",
    "Included texts are from freely available sources like Project Gutenberg and Wikipedia. They have not been vetted for textual accuracy relative to, say, a scholarly edition.\n",
    "\n",
    "**Data Description:**\n",
    "\n",
    "This lesson uses textual data in .txt format (utf-8 encoding) from various sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edaa2",
   "metadata": {},
   "source": [
    "# Introduction: The idea of distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0465fdd-6d00-4655-95fa-9d136e81b0e9",
   "metadata": {},
   "source": [
    "## Two kinds of distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74ac5e9-6c17-4cb1-9d2c-9292a486f2e8",
   "metadata": {},
   "source": [
    "In session 2 on Wednesday, we talked about the idea of words appearing near each other. That's one way to think about proximity. When we encounter a phrase in a text, like \"he is considered as the rightful property of some one or other of their daughters\", we can say that words like \"other\" and \"daughters\" are *near* each other in that phrase. Or think of our example of the words \"rightful\" (which only appears in the second sentence of *Pride and Prejudice*) and \"uniting\" (which doesn't appear until the last sentence). I think it's pretty intuitive to see those words as being \"far apart\"—and when it comes to KWICs, they are!\n",
    "\n",
    "In other text mining situations, though, you'll see the idea of \"distance\" used quite differently. Let's think back to the example from the session 1, on Monday. Here's the two dimensional graph of our bird terms:\n",
    "\n",
    "     |\n",
    "     |\n",
    "     |  penguin\n",
    "     |\n",
    "    \"|\n",
    "    s|                              duck\n",
    "    w|\n",
    "    i|\n",
    "    m|\n",
    "    s|\n",
    "    \"|\n",
    "     |\n",
    "     |\n",
    "     |                                               eagle\n",
    "     | \n",
    "     ------------------------------------------------------>\n",
    "                        \n",
    "                        \"flies\"\n",
    "\n",
    "If we want to measure the distance between \"duck\" and \"eagle\" on this graph, we can just draw a line from one to the other and see how long it is. That's a perfectly valid way to see how \"close\" they are to each other, and in fact it's what we mean when we talk about \"distance\" in the context of word vectors. But notice that this graph doesn't tell us anything about whether the words appear near each other *in a text*. We don't know if our corpus ever says \"eagle\" within 100 pages of \"duck\", or even in the same document. All we know is that the words are in similar ways relative to \"flies\" and \"swims\".\n",
    "\n",
    "So we have two forms of distance here:\n",
    " 1. Appearing next to each other \"on the page\" (we don't always literally have pages, but you get the idea) \n",
    " 2. The relative positions of words based on their arrangement in a lexical *space*\n",
    "\n",
    "As you might have noticed, the second kind of distance depends on the first. After all, our x axis just shows how often our bird terms appear *near* the word \"flies\" in the first sense of the term—i.e., we're seeing lots of phrases that say things like \"the eagle flies over the lake\". The same goes for the y axis and the word \"swims\". \n",
    "\n",
    "You might wonder: What's the point of this \"meta\" distance? Can't we just measure the first kind? The short answer is yes. That's exactly what we do when we look at collocates. The collocates of \"duck\" and \"eagle\" would include \"flies\". Yet the second kind of distance tells us things that the first kind can't. When we arrange our bird words in space like this, we see *how the words are used*. Eagles may never be used *near* ducks (or used near them only rarely), but we talk about a lot of similar things when we talk about either. \n",
    "\n",
    "We think of similarity in these terms all the time in real life. It's entirely plausible that you've never seen the names Emily Dickinson and Amanda Gorman in the same sentence before, but the two are obviously similar—they come up near words like \"poet\" and \"writer\" all the time. Two socialist 76ers fans who love pho, bluegrass, and *The Bachelorette* might be similar even if they live on opposite coasts, never appearing \"near\" each other in an immediate sense.\n",
    "\n",
    "You might notice that I've started talking about \"similarity\" rather than \"distance\". This is a common move when we're discussing that second kind of \"meta\" distance. I think it tracks a subtle shift in meaning compared to the first kind of distance. Though we can measure the length of a line between \"eagle\" and \"duck\" above in repeatable, quantifiable terms, we're not really in \"space\" in the ordinary sense when we do it. There is no background space in a word vector model. The dimensions are just other words; there is nothing \"outside\" the words anchoring them in place. (This has the trippy side effect that all potential points are also potential dimensions, but we don't need to go too far down that rabbit hole for now.) So what we're really seeing is how *similarly* the words are used in our texts.\n",
    "\n",
    "With that theoretical apparatus in place, we can turn to the \"repeatable, quantifiable\" part. Let's accept that the second kind of distance is meaningful. How do we measure it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca42822",
   "metadata": {},
   "source": [
    "## Measuring distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da268be-615e-48f8-ad3f-a48366ca7272",
   "metadata": {},
   "source": [
    "The easiest way to think about distance is to start with a version that most of us learned in junior high or high school. Here's the data for our bird words again:\n",
    "\n",
    "| term | \"flies\" | \"swims\" | \"hockey\" |\n",
    "| ---- | ---- | ---- | ---- |\n",
    "|penguin | 1 | 9 | 9\n",
    "|duck | 8 | 7 | 8\n",
    "|eagle | 10 | 1 | 0\n",
    "\n",
    "Let's measure the distance between \"duck\" and \"eagle\" on the two dimensional graph above. Eagle has a 10 for \"flies\", and duck has an 8, so they're 2 apart on the x axis. On the y axis, duck is 7 and eagle is 1, so they're 6 apart. You may have noticed that we've just drawn a right triangle, which means we can calculate a hypotenuse with the help of Pythagoras:\n",
    "\n",
    "a<sup>2</sup> + b<sup>2</sup> = c<sup>2</sup>\n",
    "\n",
    "or\n",
    "\n",
    "2<sup>2</sup> + 6<sup>2</sup> = 40\n",
    "\n",
    "so\n",
    "\n",
    "c = the square root of 40, or roughly 6.3.\n",
    "\n",
    "This is a nice, intuitive way to measure distance. If we scroll back up the graph above and draw a straight line from duck to eagle, it will be 6.3 units long. (Or anyway, it would be if I had drawn that graph more precisely.) However, it's not the only way to measure distance. We could, for instance, think in terms of Manhattan distance (also known as city block distance). \n",
    "\n",
    "Let's say I was near Union Square in New York City, at 15th Street and 5th Avenue. I'm meeting a friend in Chelsea, at 27th and 10th. In that part of town, the streets are a grid. In Euclidean terms, the distance I need to walk is:\n",
    "\n",
    "(27-15)<sup>2</sup> + (5-10)<sup>2</sup> = c<sup>2</sup>\n",
    "\n",
    "so c = 13.\n",
    "\n",
    "However, this measurement is useless to me, because I can't walk straight from my corner to my friend's corner—there are a bunch of buildings and stuff in the way. I can only walk on the streets, making right angle turns. That means the *real* distance I'm going to walk is given by \n",
    "\n",
    "(27-15) + (5-10)\n",
    "\n",
    "so it's 17 (five blocks west plus 12 blocks north).\n",
    "\n",
    "I mention this form of distance mainly because I think it's a pretty straightforward example of another way to think about things. We're just measuring along the dimensions that we've got, our streets and avenues. \n",
    "\n",
    "In text mining, we often use cosines when calculating distance or similarity. This is tough to draw in markdown, but if you draw a line from the origin to \"duck\", and another line to \"eagle\", you can then measure the cosine of the angle between them. The bigger the angle, the farther apart the two words are (notice above that the angle between \"penguin\" and \"eagle\" would be much larger than the angle between \"duck\" and \"eagle\"). The advantage of cosine distance over other metrics is that it doesn't vary as dramatically with absolute values. People don't say \"ptarmigan\" as *often* as they say \"pigeon\", but their relationship to other words is similar—that is, we'd expect both to be used near \"feathers\" and \"eggs\" much more often than they're near \"fins\" or \"astronauts\".[$^{1}$](#1) \n",
    "\n",
    "There's a small wrinkle with cosine. Cosines decrease as angles increase, so we actually expect a *small* cosine with a *big* angle, i.e. the cosine goes down as the difference between two terms grows larger. So we call the measure *cosine similarity* (the higher it is, the more similar the two terms are). We use 1-cosine to reflect *cosine distance*. The two measures capture exactly the same information, but the latter is more intuitive in comparison with other forms of distance like Euclidean or Manhattan.[$^{2}$](#2)\n",
    "\n",
    "One final issue before we move on to coding: What happens to these calculations when we add dimensions? Fortunately, the answer is: not much! This is easiest to see with Manhattan distance. If our friend is also on the 30th floor of a building (I think the address I gave in New York is actually a park in real life, but let's pretend), all we'd do to get the total distance is add the distance up in the air—the z axis. The same holds for Euclidean distance (if we add dimension z, **c<sup>2</sup>** would equal **a<sup>2</sup>** + **b<sup>2</sup>** + **z<sup>2</sup>**). Cosine distance works in much the same way—and since we'll just be using a Python library to calculate it, the extra dimensions won't pose any problem at all!\n",
    "\n",
    "<hr></hr>\n",
    "\n",
    "**Footnotes**\n",
    "\n",
    "1. <a id=\"1\"></a> For more on cosine vs Euclidean distance, see footnote 11 on page 252 of this paper (also a great resource on the distributional hypothesis in general): Baroni, Marco, Raffaella Bernardi, and Roberto Zamparelli. \"Frege in space: A program for compositional distributional semantics.\" *Linguistic Issues in language technology* 9 (2014): 241-346.\n",
    "2. <a id=\"2\"></a> For a nice primer on calculating cosine similarity/distance by hand, [see this site](https://www.geeksforgeeks.org/cosine-similarity/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c9736",
   "metadata": {},
   "source": [
    "# Practicing measuring distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d1e71-848c-4359-b318-29b97c34b145",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "We'll be using the Scipy library to do our distance calculations. Let's see it in action for a few simple examples. \n",
    "\n",
    "Imagine we have two points on a graph. We can depict them as lists:\n",
    "\n",
    "```Python\n",
    "x = [2,3]\n",
    "y = [5,7]\n",
    "```\n",
    "\n",
    "That might look something like this:\n",
    "\n",
    "     |\n",
    "     |\n",
    "     |\n",
    "     |             y  \n",
    "     |\n",
    "     |\n",
    "     |    x\n",
    "     |\n",
    "     |\n",
    "     |__ __ __ __ __ __ __ __ __ __\n",
    "\n",
    "The horizontal distance is 3, and the vertical distance is 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8cbbd1-a42e-4eb7-9b21-4d80b0900d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our points\n",
    "x = [2,3]\n",
    "y = [5,7]\n",
    "\n",
    "# What if y was a much more common word with a similar use pattern?\n",
    "\n",
    "# What if we had three dimensions?\n",
    "# x = [2,3,2]\n",
    "# y = [5,7,14]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2368107",
   "metadata": {},
   "source": [
    "# Gathering/arranging data from our corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d66db-d90e-43e8-ad79-b0edbd67c4e6",
   "metadata": {},
   "source": [
    "Before we calculate any distances, we're going to need some data. Let's take a detour back to material from earlier this week, and figure out the collocates of words in our jazz corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927c0f7-2db9-40af-911e-a4136588d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are a few useful functions we've built together\n",
    "\n",
    "# Takes a directory and returns a list of files of the specified type\n",
    "def dir2files(somedir,file_type='.txt',path=True):\n",
    "    files = os.listdir(somedir)\n",
    "    if path:\n",
    "        files = [os.path.join(somedir,f) for f in files]\n",
    "    files = [f for f in files if f.endswith(file_type)]\n",
    "    return files\n",
    "\n",
    "# Takes a word and returns a cleaned up version of it\n",
    "# This works a little differently than the version from Monday\n",
    "def cleanword(someword,remove_apostrophe=True):\n",
    "    word = someword.casefold()\n",
    "    while word and not word[0].isalpha():\n",
    "        word = word[1:]\n",
    "    while word and not word[-1].isalpha():\n",
    "        word = word[:-1]\n",
    "    return word\n",
    "\n",
    "# Takes a txt filename and returns a list of its words\n",
    "def file2words(somefilename,clean = True):\n",
    "    with open(somefilename) as source:\n",
    "        text = source.read()\n",
    "    words = text.split()\n",
    "    if clean:\n",
    "        words = [cleanword(w) for w in words]\n",
    "    return words\n",
    "\n",
    "# Takes a count dictionary and returns a sorted list of its key,value pairs\n",
    "def sortdict(somecountdict,reverse_direction=True):\n",
    "    s = sorted(somecountdict,key = lambda i:somecountdict[i],reverse=reverse_direction)\n",
    "    sorted_counts = [(word,somecountdict[word]) for word in s]\n",
    "    return sorted_counts\n",
    "\n",
    "# Takes a target term and find its KWICs in some list of words\n",
    "def get_KWIC(sometargetterm,somewords,window = 10,excl_target = True):\n",
    "    KWIC = []\n",
    "    for n,word in enumerate(somewords):\n",
    "        if word == sometargetterm:\n",
    "            start = max(n-window,0)\n",
    "            end = min(n+window+1,len(somewords)-1)\n",
    "            if excl_target:\n",
    "                k = somewords[start:n] + somewords[n+1:end]\n",
    "            else:\n",
    "                k = somewords[start:end]\n",
    "            KWIC.append(k)\n",
    "    return KWIC\n",
    "\n",
    "# Take a list of KWICs and return a dictionary of word counts\n",
    "def kwic2coll(somekwiclist):\n",
    "    counts = {}\n",
    "    for k in somekwiclist:\n",
    "        for w in k:\n",
    "            if w not in counts:\n",
    "                counts[w] = 0\n",
    "            counts[w] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb741c1c-1a7a-4bde-b128-758df2761cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin by counting all of the words in our corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fae27b-8d34-4a54-bddc-3bbfb87c6591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing all of the words in order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce9a428-21e7-463e-9ece-52b0e6e6f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at a few words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44472d9b-d682-49c0-acdb-d00b443a9667",
   "metadata": {},
   "source": [
    "## Dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4494844-8e2c-4255-9fbb-bf69dd8a8c6b",
   "metadata": {},
   "source": [
    "As we've seen, building out our array to show how every word relates to every single other word leaves us with a table that is both unwieldy and sparse (a term of art for \"filled with zeroes\"). We can solve both problems at once by making a smaller table, trading off some comprehensiveness to gain some efficiency. Since we're basically chopping dimensions off of our table, we can call this process \"dimension reduction\".\n",
    "\n",
    "This is where many word vector models take a leap in complexity. You can imagine many ways to try to maximize the information from the dimensions we keep, while minimizing the number we need. Starting small, we could *lemmatize* our data. Perhaps \"am\", \"is\", \"are\", \"was\", and \"were\" could all be reduced to \"be\". Or we could look for words that seem to correlate with each other and group them together—perhaps using the collocate data that we're collecting anyway. Or we can group words based on the way they influence the relative probabilities of the presence of other words in a series of windows across a text (a rough approximation of what Google does in the Continuous Bag of Words archictecture for word2vec).\n",
    "\n",
    "We're not going to do any of that. We want a simple, interpretable, computationally tractable model that can tell us things about our words, so we're going to approach this with two very simple assumptions:\n",
    "\n",
    "1. The most common words do the most work in a corpus\n",
    "2. Stop words aren't very semantically informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c3177-199e-4704-a987-148e819babf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide how many dimensions we want\n",
    "\n",
    "# Get a list of stopwords\n",
    "\n",
    "# Get the right number of common words that aren't stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c24475-4b8d-4488-af37-34bbeb60cd41",
   "metadata": {},
   "source": [
    "# Making the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6152cda1-7c16-4c1f-8f1e-051aeccb121e",
   "metadata": {},
   "source": [
    "## Building the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0b16e-df3d-4639-be7b-73a1f570813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, filter words\n",
    "\n",
    "# Optionally, filter stops\n",
    "\n",
    "# Gather the data\n",
    "\n",
    "# Build the array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ccd743-2d76-4e4e-8cb4-7d247efe123a",
   "metadata": {},
   "source": [
    "## Exploring the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b398f255-8d52-4df9-93a6-960839c0c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For any given word, gather its distance to every other word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e439053-35d6-4071-bffc-9d0666b28859",
   "metadata": {},
   "source": [
    "### A note on window size\n",
    "\n",
    "Here's an interesting passage from the Baroni, Bernardi, and Zamparelli piece cited in footnote 1, above:\n",
    "\n",
    "\"Different contexts capture different kinds of semantic similarity or “relatedness” (Budanitsky and Hirst 2006). At the two extremes, counting documents as contexts captures “topical” relations (the words *war* and *Afghanistan* will have a high cosine, because they often co-occur in documents), whereas DSMs based on word co-occurrence within narrow windows or constrained by syntactic relations tend to capture tighter “taxonomic” relations (such as the one between *dog* and *hyena*). Unsurprisingly, no single definition of context is appropriate for all tasks, and the jury on the “best” context model is still out (Sahlgren 2008).\" (251)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7b4f4b-b752-4770-bf3f-7aec5ddb8099",
   "metadata": {},
   "source": [
    "### Why 'verification'?\n",
    "\n",
    "With some window sizes, the word \"verification\" appears to be highly similar to our target terms, even when this seems at odds with reality (is \"verification\" much like \"swing\"? is a question I don't really know how to parse). This happens for a few other words sometimes, too, such as \"citations\". This is an oddity of our method. The word \"verification appears in our corpus exclusively in Wikipedia boilerplate that looks like this:\n",
    "\n",
    "    This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources in this section. Unsourced material may be challenged and removed. Find sources: \"Paul Motian\" – news · newspapers · books · scholar · JSTOR (March 2024) (Learn how and when to remove this message)\n",
    "\n",
    "If the window size is small enough, \"verification\" will never appear near any of the words that form the dimensions of our array. As a result, its vector will be a series of 0's—it will be located at the origin of our many-dimensional space. This causes problems for spatial's distance calculation, and results in a value of 0.\n",
    "\n",
    "How should we deal with this? Clean up our corpus? Change our dimension reduction approach? Filter our results to remove these oddballs? There are many defensible options depending on our goals and our willingness to add caveats to our final output. In the meantime, this is a nice reminder of the limitations of our small model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1173e5-3425-4e52-aae2-010192a37bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
